from typing import List, Dict, Any
from datetime import datetime
from .query_processor import QueryProcessor

class RAGEngine:
    """RAGæ ¸å¿ƒå¼•æ“Ž"""
    
    def __init__(self, notion_client, text_processor, embedder, vector_store, settings):
        self.notion_client = notion_client
        self.text_processor = text_processor
        self.embedder = embedder
        self.vector_store = vector_store
        self.settings = settings
        
        # è¨­å®šOpenAI
        if settings.USE_OPENAI and settings.OPENAI_API_KEY:
            try:
                from openai import OpenAI
                self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
                self.use_openai = True
                print(f"âœ… OpenAI API å·²è¨­å®šï¼Œæ¨¡åž‹: {settings.OPENAI_MODEL}")
            except ImportError:
                print("âŒ OpenAI å¥—ä»¶æœªå®‰è£ï¼Œä½¿ç”¨ç°¡å–®å›žæ‡‰")
                self.use_openai = False
            except Exception as e:
                print(f"âŒ OpenAI è¨­å®šå¤±æ•—: {e}")
                self.use_openai = False
        else:
            self.use_openai = False
            print("âš ï¸ æœªè¨­å®šOpenAI APIï¼Œå°‡ä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬çµ„åˆå›žæ‡‰")
        
        # åˆå§‹åŒ–æŸ¥è©¢è™•ç†å™¨
        self.query_processor = QueryProcessor(self.openai_client if self.use_openai else None)
    
    def process_notion_page(self, page_id: str) -> bool:
        """è™•ç†Notioné é¢ä¸¦åŠ å…¥å‘é‡è³‡æ–™åº«"""
        try:
            print(f"ðŸ“„ é–‹å§‹è™•ç†Notioné é¢: {page_id}")
            
            # ç²å–é é¢å…§å®¹
            print("ðŸ”„ ç²å–é é¢å…§å®¹...")
            raw_text = self.notion_client.get_page_content(page_id)
            
            # æ¸…ç†å’Œåˆ†å‰²æ–‡æœ¬
            print("ðŸ§¹ æ¸…ç†å’Œåˆ†å‰²æ–‡æœ¬...")
            cleaned_text = self.text_processor.clean_text(raw_text)
            chunks = self.text_processor.split_text(cleaned_text)
            
            print(f"âœ‚ï¸ æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} å€‹ç‰‡æ®µ")
            
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒä¾†æºçš„è³‡æ–™
            source_name = f"notion_page_{page_id}"
            existing_stats = self.vector_store.get_stats()
            
            if source_name in existing_stats.get('source_stats', {}):
                print(f"âš ï¸ ç™¼ç¾ç¾æœ‰è³‡æ–™ï¼Œå°‡æ¸…ç©ºå¾Œé‡æ–°è™•ç†")
                self.vector_store.clear_database()
            
            # ç”Ÿæˆå‘é‡åµŒå…¥
            print("ðŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = self.embedder.encode(chunks)
            
            # å„²å­˜åˆ°å‘é‡è³‡æ–™åº«
            print("ðŸ’¾ å„²å­˜åˆ°å‘é‡è³‡æ–™åº«...")
            self.vector_store.add_documents(chunks, embeddings, source_name)
            
            # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
            final_stats = self.vector_store.get_stats()
            print(f"âœ… è™•ç†å®Œæˆï¼")
            print(f"ðŸ“Š æœ€çµ‚çµ±è¨ˆ: {final_stats['total_documents']} å€‹æ–‡æª”ç‰‡æ®µ")
            
            return True
        
        except Exception as e:
            print(f"âŒ è™•ç†Notioné é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def query(self, question: str) -> str:
        """å›žç­”å•é¡Œ"""
        try:
            print(f"ðŸ¤” è™•ç†å•é¡Œ: {question}")
            
            # ä½¿ç”¨æŸ¥è©¢è™•ç†å™¨åˆ†æžå•é¡Œ
            query_analysis = self.query_processor.process_query(question)
            print(f"ðŸ“Š æŸ¥è©¢åˆ†æžçµæžœ:")
            print(f"  - æ„åœ–: {query_analysis.intent}")
            print(f"  - é—œéµè©ž: {query_analysis.keywords}")
            print(f"  - ç½®ä¿¡åº¦: {query_analysis.confidence:.2f}")
            print(f"  - æœå°‹æ¬Šé‡: {query_analysis.search_weights}")
            
            # å¤šéšŽæ®µæª¢ç´¢
            semantic_docs = []
            keyword_docs = []
            
            # 1. èªžç¾©æœå°‹
            semantic_weight = query_analysis.search_weights.get("semantic", query_analysis.search_weights.get("semantic_search", 0))
            keyword_weight = query_analysis.search_weights.get("keyword", query_analysis.search_weights.get("keyword_search", 0))
            if semantic_weight > 0:
                print("ðŸ” åŸ·è¡Œèªžç¾©æœå°‹...")
                for rewritten_query in query_analysis.rewritten_queries:
                    question_embedding = self.embedder.encode_single(rewritten_query)
                    docs = self.vector_store.search(
                        question_embedding,
                        top_k=self.settings.TOP_K
                    )
                    semantic_docs.extend(docs)
            
            # 2. é—œéµå­—æœå°‹
            if keyword_weight > 0:
                print("ðŸ” åŸ·è¡Œé—œéµå­—æœå°‹...")
                for keyword in query_analysis.keywords:
                    keyword_embedding = self.embedder.encode_single(keyword)
                    docs = self.vector_store.search(
                        keyword_embedding,
                        top_k=self.settings.TOP_K
                    )
                    keyword_docs.extend(docs)
            
            # 3. åˆä½µå’ŒåŽ»é‡æ–‡æª”
            all_docs = []
            seen_contents = set()
            
            # è™•ç†èªžç¾©æœå°‹çµæžœ
            for doc in semantic_docs:
                if doc['content'] not in seen_contents:
                    seen_contents.add(doc['content'])
                    doc['score'] *= semantic_weight
                    all_docs.append(doc)
            
            # è™•ç†é—œéµå­—æœå°‹çµæžœ
            for doc in keyword_docs:
                if doc['content'] not in seen_contents:
                    seen_contents.add(doc['content'])
                    doc['score'] *= keyword_weight
                    all_docs.append(doc)
                else:
                    # å¦‚æžœæ–‡æª”å·²å­˜åœ¨ï¼Œæ›´æ–°åˆ†æ•¸
                    for existing_doc in all_docs:
                        if existing_doc['content'] == doc['content']:
                            existing_doc['score'] += doc['score'] * keyword_weight
            
            # æŒ‰ç¶œåˆåˆ†æ•¸æŽ’åº
            all_docs.sort(key=lambda x: x['score'], reverse=True)
            relevant_docs = all_docs[:self.settings.TOP_K]
            
            if not relevant_docs:
                return "æŠ±æ­‰ï¼Œæˆ‘åœ¨ä½ çš„Notionæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚"
            
            print(f"ðŸ“‹ æ‰¾åˆ° {len(relevant_docs)} å€‹ç›¸é—œæ–‡ä»¶")
            for i, doc in enumerate(relevant_docs):
                print(f"  {i+1}. ç¶œåˆåˆ†æ•¸: {doc['score']:.3f}")
            
            # çµ„åˆä¸Šä¸‹æ–‡
            context = self._build_context(relevant_docs)
            
            # ç”Ÿæˆå›žç­”
            if self.use_openai:
                answer = self._generate_openai_response(question, context, query_analysis)
            else:
                answer = self._generate_simple_response(question, context)
            
            return answer
            
        except Exception as e:
            print(f"âŒ è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"æŠ±æ­‰ï¼Œè™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    def _calculate_keyword_score(self, query: str, content: str) -> float:
        """è¨ˆç®—é—œéµè©žåŒ¹é…åˆ†æ•¸"""
        try:
            # å°‡æŸ¥è©¢å’Œå…§å®¹è½‰æ›ç‚ºå°å¯«
            query = query.lower()
            content = content.lower()
            
            # åˆ†è©žï¼ˆç°¡å–®å¯¦ç¾ï¼Œå¯¦éš›å¯ä»¥ä½¿ç”¨æ›´è¤‡é›œçš„åˆ†è©žå™¨ï¼‰
            query_words = set(query.split())
            content_words = set(content.split())
            
            # è¨ˆç®—é—œéµè©žåŒ¹é…åº¦
            if not query_words:
                return 0.0
                
            matched_words = query_words.intersection(content_words)
            return len(matched_words) / len(query_words)
            
        except Exception as e:
            print(f"âŒ é—œéµè©žåˆ†æ•¸è¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    def _build_context(self, relevant_docs: List[Dict[str, Any]]) -> str:
        """å»ºç«‹ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, doc in enumerate(relevant_docs):
            # æ·»åŠ æ›´å¤šå…ƒè³‡è¨Š
            context_parts.append(
                f"åƒè€ƒè³‡æ–™ {i+1} (ç¶œåˆåˆ†æ•¸: {doc['score']:.3f}):\n"
                f"ä¾†æº: {doc['source']}\n"
                f"å…§å®¹: {doc['content']}\n"
                f"æ™‚é–“: {doc['created_at']}"
            )
        
        return "\n\n".join(context_parts)
    
    def _generate_openai_response(self, question: str, context: str, query_analysis) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆå›žç­”"""
        try:
            prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ï¼Œå°ˆé–€å›žç­”é—œæ–¼Notionæ–‡ä»¶å…§å®¹çš„å•é¡Œã€‚è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. æŸ¥è©¢æ„åœ–ç†è§£ï¼š
   - å·²è­˜åˆ¥çš„æŸ¥è©¢æ„åœ–ï¼š{query_analysis.intent}
   - é—œéµè©žï¼š{', '.join(query_analysis.keywords)}
   - å¯¦é«”ä¿¡æ¯ï¼š{query_analysis.entities}
   - åˆ†æžç½®ä¿¡åº¦ï¼š{query_analysis.confidence}
   - æœå°‹æ¬Šé‡é…ç½®ï¼š{query_analysis.search_weights}

2. å›žç­”ç”Ÿæˆï¼š
   - åªåŸºæ–¼æä¾›çš„åƒè€ƒè³‡æ–™ä¾†å›žç­”
   - å¦‚æžœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜Žç¢ºèªªæ˜Ž
   - ç”¨ç¹é«”ä¸­æ–‡å›žç­”
   - å›žç­”è¦ç°¡æ½”æ˜Žçž­ï¼Œé‡é»žçªå‡º
   - æ ¹æ“šæŸ¥è©¢æ„åœ–èª¿æ•´å›žç­”é¢¨æ ¼ï¼š
     * äº‹å¯¦æ€§æŸ¥è©¢ï¼šç›´æŽ¥ã€æº–ç¢º
     * æ¯”è¼ƒæ€§æŸ¥è©¢ï¼šå°æ¯”åˆ†æž
     * æ™‚é–“ç›¸é—œæŸ¥è©¢ï¼šæ™‚é–“é †åº
     * åœ°é»žç›¸é—œæŸ¥è©¢ï¼šç©ºé–“é—œä¿‚
     * ç¨‹åºæ€§æŸ¥è©¢ï¼šæ­¥é©Ÿæ¸…æ™°
     * æ¦‚å¿µæ€§æŸ¥è©¢ï¼šæ·±å…¥è§£é‡‹
   - ä¿æŒå°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§
   - é©ç•¶å¼•ç”¨åƒè€ƒè³‡æ–™ä¸­çš„å…·é«”å…§å®¹
   - å¦‚æžœä¿¡æ¯ä¸å®Œæ•´ï¼Œè«‹èªªæ˜Žå±€é™æ€§

åŽŸå§‹å•é¡Œï¼š{question}

åƒè€ƒè³‡æ–™ï¼š
{context}

è«‹æŒ‰ç…§ä¸Šè¿°æ­¥é©Ÿè™•ç†ä¸¦å›žç­”å•é¡Œã€‚å›žç­”æ™‚è«‹æ³¨æ„ï¼š
1. ç¢ºä¿å›žç­”çš„æº–ç¢ºæ€§å’Œå®Œæ•´æ€§
2. é©ç•¶å¼•ç”¨åƒè€ƒè³‡æ–™ä¸­çš„å…·é«”å…§å®¹
3. å¦‚æžœä¿¡æ¯ä¸è¶³ï¼Œè«‹èªªæ˜Žå±€é™æ€§
4. ä¿æŒå°ˆæ¥­ã€å®¢è§€çš„èªžæ°£
5. ä½¿ç”¨æ¸…æ™°çš„çµæ§‹çµ„ç¹”å›žç­”"""

            response = self.openai_client.chat.completions.create(
                model=self.settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ï¼Œå°ˆé–€å›žç­”é—œæ–¼Notionæ–‡ä»¶å…§å®¹çš„å•é¡Œã€‚
è«‹ç”¨ç¹é«”ä¸­æ–‡å›žç­”ï¼Œä¸¦ä¸”åªåŸºæ–¼æä¾›çš„è³‡æ–™ä¾†å›žç­”ã€‚
ä½ å…·æœ‰å¼·å¤§çš„èªžç¾©ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥è™•ç†ï¼š
- éŒ¯å­—å’ŒåŒç¾©è©ž
- ä¸­è‹±æ··ç”¨çš„æŸ¥è©¢
- ä¸æ™‚æ…‹å’Œèªžæ°£çš„æå•
- æ¨¡ç³Šæˆ–é–“æŽ¥çš„å•é¡Œè¡¨é”
- ä¸Šä¸‹æ–‡ç›¸é—œçš„æŸ¥è©¢
- éš±å«çš„éœ€æ±‚å’Œæ„åœ–
- å¤šå±¤æ¬¡çš„èªžç¾©ç†è§£"""},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ OpenAI å›žæ‡‰ç”Ÿæˆå¤±æ•—: {e}")
            return self._generate_simple_response(question, context)
    
    def _generate_simple_response(self, question: str, context: str) -> str:
        """ç”Ÿæˆç°¡å–®å›žæ‡‰ï¼ˆä¸ä½¿ç”¨OpenAIæ™‚çš„å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        response_parts = [
            f"åŸºæ–¼ä½ çš„Notionå…§å®¹ï¼Œæˆ‘æ‰¾åˆ°ä»¥ä¸‹ç›¸é—œè³‡è¨Šä¾†å›žç­”ã€Œ{question}ã€ï¼š",
            "",
            context,
            "",
            "ä»¥ä¸Šæ˜¯å¾žä½ çš„Notionæ–‡ä»¶ä¸­æ‰¾åˆ°çš„ç›¸é—œå…§å®¹ã€‚"
        ]
        
        return "\n".join(response_parts)
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        stats = self.vector_store.get_stats()
        
        return {
            "vector_database": {
                "total_documents": stats['total_documents'],
                "total_vectors": stats['total_vectors'],
                "sources": stats['source_stats']
            },
            "embedding_model": self.settings.EMBEDDING_MODEL,
            "openai_enabled": self.use_openai,
            "openai_model": self.settings.OPENAI_MODEL if self.use_openai else None,
            "settings": {
                "chunk_size": self.settings.CHUNK_SIZE,
                "chunk_overlap": self.settings.CHUNK_OVERLAP,
                "top_k": self.settings.TOP_K,
                "similarity_threshold": self.settings.SIMILARITY_THRESHOLD
            }
        }
    
    def update_notion_content(self, page_id: str = None) -> bool:
        """æ›´æ–°Notionå…§å®¹"""
        if page_id is None:
            page_id = self.settings.NOTION_PAGE_ID
        
        print("ðŸ”„ æ›´æ–°Notionå…§å®¹...")
        return self.process_notion_page(page_id)