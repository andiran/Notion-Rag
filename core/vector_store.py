import faiss
import sqlite3
import pickle
import os
from typing import List, Tuple, Dict, Any
import numpy as np
from datetime import datetime

class VectorStore:
    """å‘é‡è³‡æ–™åº«"""
    
    def __init__(self, vector_db_path: str, metadata_db_path: str, dimension: int = 384):
        self.vector_db_path = vector_db_path
        self.metadata_db_path = metadata_db_path
        self.dimension = dimension
        
        print(f"ğŸ—„ï¸ åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
        print(f"  å‘é‡è³‡æ–™åº«è·¯å¾‘: {vector_db_path}")
        print(f"  å…ƒè³‡æ–™åº«è·¯å¾‘: {metadata_db_path}")
        print(f"  å‘é‡ç¶­åº¦: {dimension}")
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å…§ç©ç›¸ä¼¼åº¦
        
        # åˆå§‹åŒ–SQLiteå…ƒè³‡æ–™åº«
        self._init_metadata_db()
        
        # è¼‰å…¥ç¾æœ‰è³‡æ–™
        self._load_existing_data()
        
        print(f"âœ… å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
        print(f"  ç•¶å‰å‘é‡æ•¸é‡: {self.index.ntotal}")
    
    def _init_metadata_db(self):
        """åˆå§‹åŒ–å…ƒè³‡æ–™åº«"""
        os.makedirs(os.path.dirname(self.metadata_db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_id TEXT UNIQUE,
                content TEXT,
                source TEXT,
                chunk_index INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å»ºç«‹ç´¢å¼•æå‡æŸ¥è©¢æ•ˆèƒ½
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_id ON documents(chunk_id)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_source ON documents(source)
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… å…ƒè³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
    
    def add_documents(self, texts: List[str], embeddings: np.ndarray, source: str = "notion"):
        """æ·»åŠ æ–‡ä»¶åˆ°å‘é‡è³‡æ–™åº«"""
        if len(texts) != len(embeddings):
            raise ValueError(f"æ–‡æœ¬æ•¸é‡({len(texts)})èˆ‡åµŒå…¥æ•¸é‡({len(embeddings)})ä¸åŒ¹é…")
        
        print(f"ğŸ“ æ·»åŠ  {len(texts)} å€‹æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«...")
        
        # æ­£è¦åŒ–å‘é‡ï¼ˆå°æ–¼å…§ç©ç›¸ä¼¼åº¦å¾ˆé‡è¦ï¼‰
        faiss.normalize_L2(embeddings)
        
        # ç²å–ç•¶å‰ç´¢å¼•æ•¸é‡ï¼ˆç”¨æ–¼è¨ˆç®—æ–°çš„å‘é‡IDï¼‰
        start_vector_id = self.index.ntotal
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.index.add(embeddings)
        
        # æ·»åŠ å…ƒè³‡æ–™åˆ°SQLite
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        
        for i, text in enumerate(texts):
            chunk_id = f"{source}_{start_vector_id + i}_{hash(text) % 100000}"
            cursor.execute('''
                INSERT OR REPLACE INTO documents 
                (chunk_id, content, source, chunk_index, updated_at)
                VALUES (?, ?, ?, ?, datetime('now'))
            ''', (chunk_id, text, source, start_vector_id + i))
        
        conn.commit()
        conn.close()
        
        # å„²å­˜FAISSç´¢å¼•
        self._save_faiss_index()
        
        print(f"âœ… æ–‡æª”æ·»åŠ å®Œæˆï¼Œç¸½å‘é‡æ•¸: {self.index.ntotal}")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœå°‹ç›¸ä¼¼æ–‡ä»¶"""
        if self.index.ntotal == 0:
            print("âš ï¸ å‘é‡è³‡æ–™åº«ç‚ºç©º")
            return []
        
        query_embedding = query_embedding.reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        
        # æœå°‹
        scores, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
        
        # ç²å–å°æ‡‰çš„æ–‡æœ¬å…§å®¹
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx == -1:  # FAISSè¿”å›-1è¡¨ç¤ºç„¡æ•ˆç´¢å¼•
                continue
                
            cursor.execute('''
                SELECT content, source, chunk_id, created_at 
                FROM documents 
                WHERE chunk_index = ?
            ''', (int(idx),))
            
            row = cursor.fetchone()
            if row:
                results.append({
                    'content': row[0],
                    'source': row[1],
                    'chunk_id': row[2],
                    'created_at': row[3],
                    'score': float(scores[0][i]),
                    'index': int(idx)
                })
        
        conn.close()
        
        # æŒ‰åˆ†æ•¸æ’åºï¼ˆåˆ†æ•¸è¶Šé«˜è¶Šç›¸ä¼¼ï¼‰
        results.sort(key=lambda x: x['score'], reverse=True)
        
        return results
    
    def get_all_documents(self) -> List[Dict[str, Any]]:
        """ç²å–æ‰€æœ‰æ–‡æª”"""
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT chunk_id, content, source, chunk_index, created_at, updated_at
            FROM documents 
            ORDER BY chunk_index
        ''')
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'chunk_id': row[0],
                'content': row[1],
                'source': row[2],
                'chunk_index': row[3],
                'created_at': row[4],
                'updated_at': row[5]
            })
        
        conn.close()
        return results
    
    def clear_database(self):
        """æ¸…ç©ºè³‡æ–™åº«"""
        print("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡è³‡æ–™åº«...")
        
        # é‡æ–°åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dimension)
        
        # æ¸…ç©ºSQLite
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        cursor.execute('DELETE FROM documents')
        conn.commit()
        conn.close()
        
        # åˆªé™¤FAISSæª”æ¡ˆ
        if os.path.exists(self.vector_db_path):
            os.remove(self.vector_db_path)
        
        print("âœ… è³‡æ–™åº«å·²æ¸…ç©º")
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        
        # ç¸½æ–‡æª”æ•¸
        cursor.execute('SELECT COUNT(*) FROM documents')
        doc_count = cursor.fetchone()[0]
        
        # æŒ‰ä¾†æºçµ±è¨ˆ
        cursor.execute('SELECT source, COUNT(*) FROM documents GROUP BY source')
        source_stats = dict(cursor.fetchall())
        
        # å¹³å‡æ–‡æª”é•·åº¦
        cursor.execute('SELECT AVG(LENGTH(content)) FROM documents')
        avg_length = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_documents': doc_count,
            'total_vectors': self.index.ntotal,
            'source_stats': source_stats,
            'avg_content_length': round(avg_length, 2),
            'vector_dimension': self.dimension
        }
    
    def _save_faiss_index(self):
        """å„²å­˜FAISSç´¢å¼•"""
        os.makedirs(os.path.dirname(self.vector_db_path), exist_ok=True)
        faiss.write_index(self.index, self.vector_db_path)
    
    def _load_existing_data(self):
        """è¼‰å…¥ç¾æœ‰è³‡æ–™"""
        if os.path.exists(self.vector_db_path):
            try:
                self.index = faiss.read_index(self.vector_db_path)
                print(f"âœ… è¼‰å…¥ç¾æœ‰å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {self.index.ntotal} å€‹å‘é‡")
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥å‘é‡ç´¢å¼•å¤±æ•—: {e}")
                print("å°‡å»ºç«‹æ–°çš„ç´¢å¼•")
                self.index = faiss.IndexFlatIP(self.dimension)