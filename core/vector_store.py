import faiss
import sqlite3
import pickle
import os
from typing import List, Tuple, Dict, Any
import numpy as np
from datetime import datetime
import shutil

class VectorStore:
    """å‘é‡è³‡æ–™åº«"""
    
    def __init__(self, vector_db_path: str, metadata_db_path: str, dimension: int = 384):
        self.vector_db_path = vector_db_path
        self.metadata_db_path = metadata_db_path
        self.dimension = dimension
        
        print(f"ğŸ—„ï¸ åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
        print(f"  å‘é‡è³‡æ–™åº«è·¯å¾‘: {vector_db_path}")
        print(f"  å…ƒè³‡æ–™åº«è·¯å¾‘: {metadata_db_path}")
        print(f"  å‘é‡ç¶­åº¦: {dimension}")
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å…§ç©ç›¸ä¼¼åº¦
        
        # åˆå§‹åŒ–SQLiteå…ƒè³‡æ–™åº«
        self._init_metadata_db()
        
        # è¼‰å…¥ç¾æœ‰è³‡æ–™
        self._load_existing_data()
        
        print(f"âœ… å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
        print(f"  ç•¶å‰å‘é‡æ•¸é‡: {self.index.ntotal}")
    
    def _init_metadata_db(self):
        """åˆå§‹åŒ–å…ƒè³‡æ–™åº«"""
        # å¦‚æœæ˜¯ :memory: è·¯å¾‘ï¼Œè·³éå‰µå»ºç›®éŒ„
        if self.metadata_db_path != ":memory:":
            os.makedirs(os.path.dirname(self.metadata_db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_id TEXT UNIQUE,
                content TEXT,
                source TEXT,
                chunk_index INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å»ºç«‹ç´¢å¼•æå‡æŸ¥è©¢æ•ˆèƒ½
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_id ON documents(chunk_id)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_source ON documents(source)
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… å…ƒè³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
    
    def add_documents(self, texts: List[str], embeddings: np.ndarray, source: str = "notion"):
        """æ·»åŠ æ–‡ä»¶åˆ°å‘é‡è³‡æ–™åº«"""
        if len(texts) != len(embeddings):
            raise ValueError(f"æ–‡æœ¬æ•¸é‡({len(texts)})èˆ‡åµŒå…¥æ•¸é‡({len(embeddings)})ä¸åŒ¹é…")
        
        print(f"ğŸ“ æ·»åŠ  {len(texts)} å€‹æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«...")
        
        # æ­£è¦åŒ–å‘é‡ï¼ˆå°æ–¼å…§ç©ç›¸ä¼¼åº¦å¾ˆé‡è¦ï¼‰
        faiss.normalize_L2(embeddings)
        
        # ç²å–ç•¶å‰ç´¢å¼•æ•¸é‡ï¼ˆç”¨æ–¼è¨ˆç®—æ–°çš„å‘é‡IDï¼‰
        start_vector_id = self.index.ntotal
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.index.add(embeddings)
        
        # æ·»åŠ å…ƒè³‡æ–™åˆ°SQLite
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        
        for i, text in enumerate(texts):
            chunk_id = f"{source}_{start_vector_id + i}_{hash(text) % 100000}"
            cursor.execute('''
                INSERT OR REPLACE INTO documents 
                (chunk_id, content, source, chunk_index, updated_at)
                VALUES (?, ?, ?, ?, datetime('now'))
            ''', (chunk_id, text, source, start_vector_id + i))
        
        conn.commit()
        conn.close()
        
        # å„²å­˜FAISSç´¢å¼•
        self._save_faiss_index()
        
        print(f"âœ… æ–‡æª”æ·»åŠ å®Œæˆï¼Œç¸½å‘é‡æ•¸: {self.index.ntotal}")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 5, settings: Dict = None, _recursion_depth: int = 0) -> List[Dict[str, Any]]:
        """æœå°‹ç›¸ä¼¼æ–‡ä»¶
        Args:
            query_embedding: æŸ¥è©¢å‘é‡
            top_k: è¿”å›çµæœæ•¸é‡
            settings: ç›¸ä¼¼åº¦è¨­å®šï¼ˆå¯é¸ï¼‰
            _recursion_depth: éè¿´æ·±åº¦ï¼ˆå…§éƒ¨ç”¨ï¼‰
        """
        if self.index.ntotal == 0:
            print("âš ï¸ å‘é‡è³‡æ–™åº«ç‚ºç©ºï¼ˆé˜²å‘†æç¤ºï¼‰")
            return [{
                'content': 'ç›®å‰è³‡æ–™åº«æ²’æœ‰ä»»ä½•å…§å®¹ï¼Œè«‹å…ˆåŒæ­¥ Notion è³‡æ–™ã€‚',
                'source': '',
                'chunk_id': '',
                'created_at': '',
                'score': 0,
                'recency_score': 0,
                'length_score': 0,
                'index': -1
            }]
        
        # ä½¿ç”¨é è¨­è¨­å®šæˆ–å‚³å…¥çš„è¨­å®š
        settings = settings or {}
        base_threshold = settings.get("BASE_THRESHOLD", 0.3)
        dynamic_settings = settings.get("DYNAMIC_THRESHOLD", {})
        filter_settings = settings.get("FILTER_SETTINGS", {})
        min_threshold = dynamic_settings.get("MIN_THRESHOLD", 0.25) if dynamic_settings.get("ENABLED", False) else 0.01
        max_recursion = 5
        
        query_embedding = query_embedding.reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        
        # è¨ˆç®—å‹•æ…‹é–¾å€¼
        if dynamic_settings.get("ENABLED", False):
            all_scores, _ = self.index.search(query_embedding, self.index.ntotal)
            scores = all_scores[0]
            
            # è¨ˆç®—åˆ†æ•¸åˆ†ä½ˆ
            mean_score = np.mean(scores)
            std_score = np.std(scores)
            
            # ä½¿ç”¨åŠ æ¬Šæ–¹å¼è¨ˆç®—å‹•æ…‹é–¾å€¼
            score_distribution = dynamic_settings.get("SCORE_DISTRIBUTION", {})
            mean_weight = score_distribution.get("MEAN_WEIGHT", 0.6)
            std_weight = score_distribution.get("STD_WEIGHT", 0.4)
            
            dynamic_threshold = (
                mean_score * mean_weight + 
                (mean_score + std_score * dynamic_settings.get("ADJUSTMENT_FACTOR", 0.15)) * std_weight
            )
            
            # ç¢ºä¿é–¾å€¼åœ¨åˆç†ç¯„åœå…§
            threshold = max(
                min(dynamic_threshold, dynamic_settings.get("MAX_THRESHOLD", 0.45)),
                min_threshold
            )
        else:
            threshold = base_threshold
        
        # åŸ·è¡Œæœå°‹
        scores, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        results = []
        
        # ç²å–é•·åº¦æ‡²ç½°è¨­å®š
        length_penalty = filter_settings.get("LENGTH_PENALTY", {})
        apply_length_penalty = length_penalty.get("ENABLED", True)
        min_length = length_penalty.get("MIN_LENGTH", 10)
        max_length = length_penalty.get("MAX_LENGTH", 500)
        penalty_factor = length_penalty.get("PENALTY_FACTOR", 0.1)
        
        for i, idx in enumerate(indices[0]):
            if idx == -1:
                continue
                
            score = float(scores[0][i])
            if score < threshold:
                continue
                
            cursor.execute('''
                SELECT content, source, chunk_id, created_at 
                FROM documents 
                WHERE chunk_index = ?
            ''', (int(idx),))
            row = cursor.fetchone()
            
            if row:
                content = row[0]
                created_at = datetime.fromisoformat(row[3].replace('Z', '+00:00'))
                time_diff = datetime.now() - created_at
                
                # è¨ˆç®—æ™‚é–“è¡°æ¸›åˆ†æ•¸
                recency_score = 1.0 / (1.0 + time_diff.days * filter_settings.get("SCORE_DECAY", 0.15))
                
                # è¨ˆç®—é•·åº¦æ‡²ç½°
                length_score = 1.0
                if apply_length_penalty:
                    content_length = len(content)
                    if content_length < min_length:
                        length_score = 1.0 - (min_length - content_length) * penalty_factor
                    elif content_length > max_length:
                        length_score = 1.0 - (content_length - max_length) * penalty_factor
                
                # ç¶œåˆè©•åˆ†ï¼ˆç¢ºä¿ä¸è¶…éåŸå§‹é–¾å€¼ï¼‰
                bonus_factor = 0.1  # é¡å¤–åŠ åˆ†å› å­
                final_score = score * (1.0 + (recency_score + length_score - 1.0) * bonus_factor)
                
                # ç¢ºä¿åˆ†æ•¸ä¸è¶…éé–¾å€¼
                if dynamic_settings.get("ENABLED", False):
                    final_score = min(final_score, dynamic_settings.get("MAX_THRESHOLD", 0.45))
                else:
                    final_score = min(final_score, base_threshold)
                
                results.append({
                    'content': content,
                    'source': row[1],
                    'chunk_id': row[2],
                    'created_at': row[3],
                    'score': final_score,
                    'recency_score': recency_score,
                    'length_score': length_score,
                    'index': int(idx)
                })
        
        conn.close()
        
        # æŒ‰ç¶œåˆåˆ†æ•¸æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        
        # æ‡‰ç”¨çµæœéæ¿¾
        min_results = filter_settings.get("MIN_RESULTS", 1)
        max_results = filter_settings.get("MAX_RESULTS", 8)
        
        # éè¿´çµ‚æ­¢æ¢ä»¶ï¼š
        # 1. å·²é”æœ€å¤§éè¿´æ·±åº¦
        # 2. é–¾å€¼å·²ç¶“ä½æ–¼ min_threshold
        # 3. çµæœæ•¸å·²ç­‰æ–¼è³‡æ–™åº«ç¸½æ•¸
        if (len(results) < min_results and len(results) > 0 and
            threshold > min_threshold and _recursion_depth < max_recursion and
            len(results) < self.index.ntotal):
            return self.search(query_embedding, top_k=max_results, settings={
                **settings,
                "BASE_THRESHOLD": threshold * 0.8
            }, _recursion_depth=_recursion_depth+1)
            
        return results[:max_results]
    
    def get_all_documents(self) -> List[Dict[str, Any]]:
        """ç²å–æ‰€æœ‰æ–‡æª”"""
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT chunk_id, content, source, chunk_index, created_at, updated_at
            FROM documents 
            ORDER BY chunk_index
        ''')
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'chunk_id': row[0],
                'content': row[1],
                'source': row[2],
                'chunk_index': row[3],
                'created_at': row[4],
                'updated_at': row[5]
            })
        
        conn.close()
        return results
    
    def clear_database(self):
        """æ¸…ç©ºè³‡æ–™åº«"""
        print("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡è³‡æ–™åº«...")
        # é‡æ–°åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dimension)
        # æ¸…ç©ºSQLite
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        cursor.execute('DELETE FROM documents')
        conn.commit()
        conn.close()
        # åˆªé™¤FAISSæª”æ¡ˆæˆ–è³‡æ–™å¤¾
        if os.path.isdir(self.vector_db_path):
            shutil.rmtree(self.vector_db_path)
        elif os.path.exists(self.vector_db_path):
            os.remove(self.vector_db_path)
        print("âœ… è³‡æ–™åº«å·²æ¸…ç©º")
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
        conn = sqlite3.connect(self.metadata_db_path)
        cursor = conn.cursor()
        
        # ç¸½æ–‡æª”æ•¸
        cursor.execute('SELECT COUNT(*) FROM documents')
        doc_count = cursor.fetchone()[0]
        
        # æŒ‰ä¾†æºçµ±è¨ˆ
        cursor.execute('SELECT source, COUNT(*) FROM documents GROUP BY source')
        source_stats = dict(cursor.fetchall())
        
        # å¹³å‡æ–‡æª”é•·åº¦
        cursor.execute('SELECT AVG(LENGTH(content)) FROM documents')
        avg_length = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_documents': doc_count,
            'total_vectors': self.index.ntotal,
            'source_stats': source_stats,
            'avg_content_length': round(avg_length, 2),
            'vector_dimension': self.dimension
        }
    
    def _save_faiss_index(self):
        """å„²å­˜FAISSç´¢å¼•"""
        # å¦‚æœæ˜¯ :memory: è·¯å¾‘ï¼Œè·³éå„²å­˜
        if self.vector_db_path == ":memory:":
            return
        
        os.makedirs(os.path.dirname(self.vector_db_path), exist_ok=True)
        faiss.write_index(self.index, self.vector_db_path)
    
    def _load_existing_data(self):
        """è¼‰å…¥ç¾æœ‰è³‡æ–™"""
        if os.path.exists(self.vector_db_path):
            try:
                self.index = faiss.read_index(self.vector_db_path)
                print(f"âœ… è¼‰å…¥ç¾æœ‰å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {self.index.ntotal} å€‹å‘é‡")
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥å‘é‡ç´¢å¼•å¤±æ•—: {e}")
                print("å°‡å»ºç«‹æ–°çš„ç´¢å¼•")
                self.index = faiss.IndexFlatIP(self.dimension)